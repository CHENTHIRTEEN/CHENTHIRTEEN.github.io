---
title: 机器学习（一）
date: 2021-09-16 14:20:24
tags:
  - 机器学习
categories:
  - 机器学习
  - 线性回归
math: true
mermaid: true
---

_听课时候随手记的，整理自黄海广博士的机器学习课程 ：)_

机器学习流程：

```mermaid
graph LR;
数据搜集 --> 数据清洗 --> 特征工程 --> 数据建模
```

前三个部分日常最多 80%-90%时间

数学建模需要最多的知识

### 监督学习

- 回归问题：标签连续
- 分类问题：标签离散

## 回归

### 线性回归

- 线性模型
- 找到直线、平面、或者超平面是的预测值与真实值之间误差最小
- 距离为 0 则完全拟合

### 符号约定

- **m：训练集中的样本数量**
- **n：特征数量**
- **x：特征/输入变量**
- **y：目标变量/输出变量**
- **（x， y）：训练集中的样本**

默认为列向量

- **$x^{(i)}$：特征矩阵中的第 i 行（默认为列向量）**
- **$x^{(i)}_j$：矩阵中第 i 行第 j 个特征**
- **$h：$代表学习算法的解决方案或函数也成为假设**
- **$\widehat{y}=h(x) $：代表预测值**
- **$h：$学习算法的解决方案或函数，也成为假设**
- $\hat{y} = h(x):$ **预测的值**

#### 最小二乘法

$$
h(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

- 找到一组

$$
w(w_0, w_1, w_2,..., w_n)
$$

- 使得残差平方和

$$
J(w) = \frac{1}{2} {\textstyle \sum_{i = 1}^{m}}(h(x^{(i)}) - y^{(i)})^{2}
$$

最小，即最小化$\frac{\partial J(w)}{\partial w}.$

- 将上式转换为矩阵形式

$$
J(w) = \frac{1}{2}(Xw - Y)^{2}
$$

- 其中$X$为$m$行$n+1$列的矩阵（$m为样本个数， n为特征个数$）
- $w为n+1行1列的矩阵（包含w_0）,Y为m行1列的矩阵$

**根据向量的性质：**

$$
\sum_{i}Z_{i}^2 = Z^TZ
$$

**可以得到：**

$$
J(w) = \frac{1}{2}(Xw - Y)^{2} = \frac{1}{2}(Xw - Y)^{T}(Xw - Y)
$$

**未来最小化$J(w)$，对其求偏导：**

$$
\frac{\partial J(w)}{\partial w} = \frac{1}{2} \frac{\partial}{\partial w} (Xw - Y)^T(Xw - Y) = \frac{1}{2} \frac{\partial}{\partial w} (w^TX^TXw - {\color{Red} Y^TXw  - w^TX^TY} + Y^TY)
$$

**红色两项互为转置**

$$
\frac{\partial J(w)}{\partial w} = \frac{1}{2} \frac{\partial}{\partial w}  (w^TX^TXw - 2w^TX^TY + Y^TY) = \frac{1}{2}(2X^TXw - 2X^Ty + 0)\\=X^TXw - X^Ty
$$

$$
令 \frac{\partial J(w)}{\partial w} = 0
$$

则有

$$
w = (X^TX)^{-1}X^Ty
$$

**_注意以下法则_**

$$
\frac{dX^TX}{dX} = 2X
$$

$$
\frac{dAX}{dX}
$$

$$
\frac{\partial X^TAX}{\partial X} = (A + A^T)X,	若A为对称矩阵， \frac{\partial X^TAX}{\partial X} = 2AX
$$

---

#### 批量梯度下降

- **梯度下降的每一步都是用所有训练样本**

**_参数更新_**：（同步更新$w_j$(j = 0,1,……,n)）

$$
w_j:=w_j - \alpha \frac {1}{m} \sum_{i = 1}^{m}((h(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j)
$$

#### 随机梯度下降

##### 梯度的推导：

$$
w = w - \alpha \cdot \frac{\partial J(w)}{\partial w}
$$

**由最小二乘法：**

$$
h(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

**得到残差平方：**

$$
J(w) = \frac{1}{2} (h(x^{(i)}) - y^{(i)})^{2}
$$

$$
\frac{\partial}{\partial w_j} J(w) = \frac{\partial}{\partial w_j} \frac{1}{2}(h(x^{(i)}) - y^{(i)})^{2} = 2 \cdot \frac{1}{2}(h(x^{(i)}) - y^{(i)})\cdot \frac{\partial}{\partial w_j}(h(x^{(i)}) - y^{(i)})
$$

$$
=(h(x^{(i)}) - y^{(i)}) \cdot \frac{\partial}{\partial w_j}(\sum_{i = 0}^{n} (w_i x^{(i)}_i -y^{(i)}))
$$

$$
=(h(x^{(i)}) - y^{(i)})x^{(i)}_j
$$

**_随机梯度下降每一步用到一个样本，在每次计算中都更新参数，不需要提前对训练集求和_**

**_参数更新：_**

$$
w_j:=w_j-\alpha(h(x^{(i)}) - y^{(i)})x^{(i)}_j
$$

#### 小批量梯度下降

**_每一步用到一定批量的训练样本，每计算 b 次训练样本，更新一次参数 w_**

**_参数更新：_**

$$
w_j:=w_j - \alpha \frac{1}{b} \sum_{k = 1}^{i+b-1}(h(x^{(i)}) - y^{(i)})x^{(i)}_j
$$

**注意：**

**b = 1 (随机梯度下降，SGD)**

**b = m (批量梯度下降，BGD)**

**b = batch_size，通常为 2 的指数倍，常见为 32，64，128……（小批量梯度下降，MBGD）**

#### 总结

**最小二乘法**

- 只适用线性模型
- 特征值较多时矩阵计算代价过大（矩阵计算时间复杂度为$O(n^3)$当 n 小于 10000 可以接受）

- 不需要选择学习率$\alpha$
- 只需要一次计算

**梯度下降**

- 适用于各种模型
- 特征值较多时也适用
- 需要选择学习率
- 需要多次计算

---

#### 数据归一化

**_归一化（最大-最小规范化）_**

$$
x^{*} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

将数据映射到[0,1]区间

数据归一化的目的是使得各特征对目标变量的影响一致，将特征数据进行伸缩变化，数据归一化是会 改变特征数据分布。

#### 数据标准化

**_Z-Score 标准化_**

$$
x^{*} = \frac{x - \mu}{\sigma}
$$

**_其中：_**

$$
{\textstyle \sigma}^{2} = \frac{1}{m} \sum_{i = 1}^{m}(x^{(i)})^2
$$

$$
\mu = \frac{1}{m} \sum_{i = 1}^{m}x^{(i)}
$$

- 经过标准化后，数据均值为 0，方差为 1

- 标准化使不同特征具有可比性
- 标准化后的特征数据分布没有发生变化
- 当数据特征取值范围或单位差异较大时，需要做一下标准化处理

##### 需要数据归一化/标准化

- 线性模型
- 基于度量的模型（KNN，K-means 聚类、感知机、SVM）

##### 不需要归一化/标准化

- 决策树
- 基于决策树的集成学习模型（基于 Boosting、Bagging 等对于特征值大小不敏感的模型）例如 XGBoost、LightGBM、朴素贝叶斯等

---

### 过拟合的处理

- 更多的训练数据

使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。

- 降维

即丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA)

- 正则化

正则化技术，保留所有的特征，但是减少参数的大小（ magnitude），它可以改善或者减少过拟合问题。

- 集成学习

集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险。

### 欠拟合的处理

- 添加新特征

当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。通过挖掘组合特征等新的特征，往往能够取得更好的效果。

- 增加模型复杂度

简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力。例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数等。

- 减少正则化系数

正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要有针对性地减小正则化系数。

---

### 正则化

#### $L_1$正则化（Lasso 回归）

**$L_1$正则化在损失函数中加入权值的绝对值之和，目的是使权重稀疏，防止欠拟合**

$$
J(w) = \frac{1}{2} {\textstyle \sum_{i = 1}^{m}}(h(x^{(i)}) - y^{(i)})^{2} + \lambda {\textstyle \sum_{j = 1}^{n}}|w_j|
$$

#### $L_2$正则化（岭回归）

**$L_2$正则化在损失函数中加入权值的平方和，可以使权重平滑，防止过拟合**

$$
J(w) = \frac{1}{2} {\textstyle \sum_{i = 1}^{m}}(h(x^{(i)}) - y^{(i)})^{2} + \lambda {\textstyle \sum_{j = 1}^{n}}w_j
$$

---

### 回归的评价指标

**指标中$y^{(i)}$和$\widehat{y}^{(i)} $表示第 i 个样本的真实值和预测值，m 为样本个数**

- 均方误差（Mean Square Error,MSE）

$$
MSE = \frac{1}{m} \sum_{i=1}^{m}(y^{(i)} - \widehat{y}^{(i)})^{(2)}
$$

- 均方根误差（Root Mean Square Error，RMSE）

$$
RMSE(y,\widehat{y}) = \sqrt{\frac{1}{m} \sum_{i=1}^{m}(y^{(i)} - \widehat{y}^{(i)})^{2}}
$$

- 平均绝对误差（Mean Absolute Error，MAE）

$$
MAE(y,\widehat{y}) = \frac{1}{m} \sum_{i = 1}^{m}|y^{(i)} - \widehat{y}^{(i)}|
$$

- R 方【RSquared(r2score)】

$$
SSR = \sum_{i = 0}^{m}(\widehat{y}^{(i)} - \bar{y})^2
$$

$$
SSE = \sum_{i=0}^{m}(y^{(i)}  - \widehat{y}^{(i)})^2
$$

$$
SST = \sum_{i = 0}^{m}(y^{(i)} - \bar{y})^2
$$

$$
R^2(y,\widehat{y})=1-\frac{\sum_{i=0}^{m}\left(y^{(i)}-\widehat{y}^{(i)}\right)^{2}}{\sum_{i=0}^{m}\left(y^{(i)}-\bar{y}\right)^{2}}
$$

$$
=\frac{SSR}{SST} = 1-\frac{SSE}{SST}
$$

$$
R^2(y,\widehat{y}) = 1-\frac{\sum_{i=0}^{m}\left(y^{(i)}-\widehat{y}^{(i)}\right)^{2} / m}{\sum_{i=0}^{m}\left(y^{(i)}-\bar{y}\right)^{2} / m}
$$

$$
=1-\frac{MSE}{Var}
$$

**越接近 1 说明模型拟合越好**

